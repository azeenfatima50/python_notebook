{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e0d219",
   "metadata": {},
   "source": [
    "# üî• Hybrid Deep Learning & Gradient Boosting for Multi-Stock Time Series Forecasting: A Kaggle Grandmaster Pipeline\n",
    "\n",
    "**Alternative shorter version:**  \n",
    "*End-to-End Multi-Stock Forecasting with XGBoost, LSTM, TFT & N-BEATS*\n",
    "\n",
    "**SEO-friendly version:**  \n",
    "*Stock Price Forecasting with XGBoost, LightGBM, CatBoost, LSTM, TFT & N-BEATS ‚Äî Complete Kaggle Pipeline*\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ Notebook Overview\n",
    "\n",
    "### üîé Overview\n",
    "This notebook presents a complete end-to-end framework for **multi-stock time series forecasting** using both **classical machine learning** and **state-of-the-art deep learning architectures**.\n",
    "\n",
    "We build a production-ready forecasting pipeline covering:\n",
    "\n",
    "- Robust preprocessing & feature engineering  \n",
    "- Time-aware cross-validation (`TimeSeriesSplit`)  \n",
    "- Gradient Boosting models (**XGBoost, LightGBM, CatBoost**)  \n",
    "- Deep Learning models (**LSTM + GRU**)  \n",
    "- Advanced architectures (**Temporal Fusion Transformer & N-BEATS**)  \n",
    "- Ensemble blending strategies  \n",
    "- Kaggle-ready submission generation  \n",
    "\n",
    "The dataset includes multiple semiconductor stocks (**AMD, ASML, INTC, NVDA**), and models are trained per stock to capture individual market dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a268c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Classical\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# DL\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Bidirectional\n",
    "\n",
    "# Transformers\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, NBeats\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb47b8",
   "metadata": {},
   "source": [
    "## üßæ Code Cell ‚Äì Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1. Load CSV with Multi-Header\n",
    "# ===============================\n",
    "df = pd.read_csv(\"ai_chip_stocks_2018_2026.csv\", header=[0,1,2])\n",
    "\n",
    "# ===============================\n",
    "# 2. Fix Column Names\n",
    "# ===============================\n",
    "df.columns = [\n",
    "    f\"{c2}_{c0}\" if c1 == \"\" else f\"{c1}_{c0}\"\n",
    "    for c0, c1, c2 in df.columns\n",
    "]\n",
    "\n",
    "# First column is Date\n",
    "df.rename(columns={df.columns[0]: \"Date\"}, inplace=True)\n",
    "\n",
    "# ===============================\n",
    "# 3. Convert Date\n",
    "# ===============================\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Date\"])\n",
    "\n",
    "# ===============================\n",
    "# 4. Reshape to Long Format\n",
    "# ===============================\n",
    "tickers = [\"AMD\", \"ASML\", \"INTC\", \"NVDA\"]\n",
    "ohlcv = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for t in tickers:\n",
    "    cols = [c for c in df.columns if c.startswith(t)]\n",
    "    temp = df[[\"Date\"] + cols].copy()\n",
    "    temp.columns = [\"Date\"] + ohlcv\n",
    "    temp[\"Ticker\"] = t\n",
    "    data_list.append(temp)\n",
    "\n",
    "data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "# ===============================\n",
    "# 5. Sort & Add time_idx\n",
    "# ===============================\n",
    "data.sort_values([\"Ticker\",\"Date\"], inplace=True)\n",
    "data[\"time_idx\"] = data.groupby(\"Ticker\").cumcount()\n",
    "\n",
    "# ===============================\n",
    "# 6. Done\n",
    "# ===============================\n",
    "print(\"‚úÖ Data processed successfully\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d5dff",
   "metadata": {},
   "source": [
    "## üõ† Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da790ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 3, 7, 14, 30]:\n",
    "        data[f\"lag_{lag}\"] = data.groupby(\"Ticker\")[\"Close\"].shift(lag)\n",
    "    \n",
    "    # Rolling means (shifted to avoid leakage)\n",
    "    data[\"rolling_7\"] = data.groupby(\"Ticker\")[\"Close\"].shift(1).rolling(7).mean()\n",
    "    data[\"rolling_14\"] = data.groupby(\"Ticker\")[\"Close\"].shift(1).rolling(14).mean()\n",
    "    data[\"rolling_30\"] = data.groupby(\"Ticker\")[\"Close\"].shift(1).rolling(30).mean()\n",
    "\n",
    "    # Rolling volatility\n",
    "    data[\"volatility_7\"] = data.groupby(\"Ticker\")[\"Close\"].shift(1).rolling(7).std()\n",
    "    data[\"volatility_14\"] = data.groupby(\"Ticker\")[\"Close\"].shift(1).rolling(14).std()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply features\n",
    "data = add_features(data)\n",
    "\n",
    "# Drop rows with NaNs from lags/rolling\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0373f",
   "metadata": {},
   "source": [
    "## üîÅ TimeSeries CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dae361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# TimeSeries Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Select feature columns (lags + rolling + volatility)\n",
    "features = [c for c in df.columns \n",
    "            if c.startswith(\"lag_\") \n",
    "            or c.startswith(\"rolling_\") \n",
    "            or c.startswith(\"volatility_\")]\n",
    "\n",
    "# Target column\n",
    "target = \"Close\"\n",
    "\n",
    "print(\"Features:\", features)\n",
    "print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25535f5",
   "metadata": {},
   "source": [
    "## üå≤ XGBoost / LightGBM / CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Preprocess DataFrame columns\n",
    "# ===============================\n",
    "# Strip whitespace and lowercase all column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ===============================\n",
    "# 1. Define Models\n",
    "# ===============================\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(n_estimators=300, random_state=42),\n",
    "    \"CatBoost\": CatBoostRegressor(iterations=300, verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 2. List of Stocks\n",
    "# ===============================\n",
    "stocks = [\"amd\", \"asml\", \"intc\", \"nvda\"]\n",
    "\n",
    "# ===============================\n",
    "# 3. Cross-Validation for Each Stock\n",
    "# ===============================\n",
    "results = {}\n",
    "\n",
    "for stock in stocks:\n",
    "    print(f\"\\n===== Processing {stock.upper()} =====\")\n",
    "    \n",
    "    target_col = f\"{stock}_close\"\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Skipping {stock}: target column '{target_col}' not found\")\n",
    "        continue\n",
    "    \n",
    "    # Use all columns except the target and date as features\n",
    "    X = df[[col for col in df.columns if col != target_col and col != \"date\"]]\n",
    "    y = df[target_col]\n",
    "\n",
    "    # TimeSeries CV\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        maes = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_val)\n",
    "            maes.append(mean_absolute_error(y_val, preds))\n",
    "        \n",
    "        cv_scores[name] = np.mean(maes)\n",
    "    \n",
    "    results[stock] = cv_scores\n",
    "\n",
    "# ===============================\n",
    "# 4. Display Results\n",
    "# ===============================\n",
    "for stock, scores in results.items():\n",
    "    print(f\"\\n{stock.upper()} CV Results:\")\n",
    "    display(pd.DataFrame.from_dict(scores, orient='index', columns=['MAE']).sort_values('MAE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6a1b0",
   "metadata": {},
   "source": [
    "## ü§ñ LSTM / GRU / BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Clean column names\n",
    "# ===============================\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ===============================\n",
    "# 1. Select Target Stock\n",
    "# ===============================\n",
    "target_stock = \"amd\"   # üî• change to: \"asml\", \"intc\", \"nvda\"\n",
    "target_col = f\"{target_stock}_close\"\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"{target_col} not found in DataFrame\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Scale Target\n",
    "# ===============================\n",
    "scaler = MinMaxScaler()\n",
    "df[f\"{target_col}_scaled\"] = scaler.fit_transform(df[[target_col]])\n",
    "\n",
    "# ===============================\n",
    "# 3. Sequence Generator\n",
    "# ===============================\n",
    "def make_seq(series, window=30):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(series)):\n",
    "        X.append(series[i-window:i])\n",
    "        y.append(series[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = make_seq(df[f\"{target_col}_scaled\"].values, window=30)\n",
    "\n",
    "# Reshape for RNN input: (samples, timesteps, features)\n",
    "X_seq = X_seq.reshape((X_seq.shape[0], X_seq.shape[1], 1))\n",
    "\n",
    "# ===============================\n",
    "# 4. Build DL Model\n",
    "# ===============================\n",
    "model_dl = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(30, 1)),\n",
    "    GRU(32),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_dl.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# ===============================\n",
    "# 5. Train\n",
    "# ===============================\n",
    "model_dl.fit(X_seq, y_seq, epochs=15, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7dd12",
   "metadata": {},
   "source": [
    "## üîÆ TFT & N-BEATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ec7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Clean columns\n",
    "# ===============================\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ===============================\n",
    "# 1. Select Stock\n",
    "# ===============================\n",
    "target_stock = \"amd\"   # asml / intc / nvda\n",
    "target_col = f\"{target_stock}_close\"\n",
    "\n",
    "# ===============================\n",
    "# 2. time_idx + group\n",
    "# ===============================\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "df[\"time_idx\"] = np.arange(len(df))\n",
    "df[\"series_id\"] = target_stock\n",
    "\n",
    "# ===============================\n",
    "# 3. TFT Dataset (with features)\n",
    "# ===============================\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 30\n",
    "\n",
    "tft_dataset = TimeSeriesDataSet(\n",
    "    df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[target_col],\n",
    ")\n",
    "\n",
    "tft_loader = tft_dataset.to_dataloader(train=True, batch_size=64, num_workers=0)\n",
    "\n",
    "# ===============================\n",
    "# 4. Train TFT\n",
    "# ===============================\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    tft_dataset,\n",
    "    learning_rate=0.001,\n",
    "    loss=QuantileLoss(),\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "trainer.fit(tft, train_dataloaders=tft_loader)\n",
    "\n",
    "# ===============================\n",
    "# 5. N-BEATS Dataset (TARGET ONLY!)\n",
    "# ===============================\n",
    "nbeats_dataset = TimeSeriesDataSet(\n",
    "    df[[\"time_idx\", \"series_id\", target_col]],  # üëà ONLY target\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target_col,\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[target_col],\n",
    ")\n",
    "\n",
    "nbeats_loader = nbeats_dataset.to_dataloader(train=True, batch_size=64, num_workers=0)\n",
    "\n",
    "# ===============================\n",
    "# 6. Train N-BEATS\n",
    "# ===============================\n",
    "nbeats = NBeats.from_dataset(\n",
    "    nbeats_dataset,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "trainer.fit(nbeats, train_dataloaders=nbeats_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2609a8",
   "metadata": {},
   "source": [
    "## üßÆ Ensemble + Leaderboard Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Clean column names\n",
    "# ===============================\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ===============================\n",
    "# 1. Select Target Stock\n",
    "# ===============================\n",
    "target_stock = \"amd\"   # üîÅ asml / intc / nvda\n",
    "target_col = f\"{target_stock}_close\"\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    raise ValueError(f\"{target_col} not found in DataFrame\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Build Feature Matrix (NUMERIC ONLY)\n",
    "# ===============================\n",
    "# Drop target, date, and any object/categorical columns\n",
    "drop_cols = [target_col, \"date\", \"series_id\"]\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[features].select_dtypes(include=[\"number\"]).copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# ===============================\n",
    "# 3. Define Models\n",
    "# ===============================\n",
    "models = {\n",
    "    \"xgb\": xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"lgb\": lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"cat\": CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 4. Fit Models on FULL Data\n",
    "# ===============================\n",
    "for name, model in models.items():\n",
    "    model.fit(X, y)\n",
    "\n",
    "# ===============================\n",
    "# 5. Ensemble Predictions\n",
    "# ===============================\n",
    "pred_xgb = models[\"xgb\"].predict(X)\n",
    "pred_lgb = models[\"lgb\"].predict(X)\n",
    "pred_cat = models[\"cat\"].predict(X)\n",
    "\n",
    "# Average blend\n",
    "final_preds = (pred_xgb + pred_lgb + pred_cat) / 3\n",
    "\n",
    "# ===============================\n",
    "# 6. Leaderboard Trick\n",
    "# ===============================\n",
    "final_preds = np.maximum(final_preds, 0)\n",
    "\n",
    "print(\"Final predictions shape:\", final_preds.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
